{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling monosemanticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venvllm/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in ./venvllm/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venvllm/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: requests in ./venvllm/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venvllm/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvllm/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venvllm/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in ./venvllm/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venvllm/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: xxhash in ./venvllm/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pandas in ./venvllm/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in ./venvllm/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./venvllm/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venvllm/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venvllm/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venvllm/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venvllm/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: keras in ./venvllm/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in ./venvllm/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: h5py in ./venvllm/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: packaging in ./venvllm/lib/python3.10/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: optree in ./venvllm/lib/python3.10/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: absl-py in ./venvllm/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: ml-dtypes in ./venvllm/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: numpy in ./venvllm/lib/python3.10/site-packages (from keras) (1.26.3)\n",
      "Requirement already satisfied: namex in ./venvllm/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: rich in ./venvllm/lib/python3.10/site-packages (from keras) (12.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: requests in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: filelock in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras) (0.9.1)\n",
      "Requirement already satisfied: tensorflow in ./venvllm/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: setuptools in ./venvllm/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: packaging in ./venvllm/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvllm/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: optree in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: namex in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: rich in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (12.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvllm/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (0.9.1)\n",
      "Requirement already satisfied: python-dotenv in ./venvllm/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: zstandard in ./venvllm/lib/python3.10/site-packages (0.23.0)\n"
     ]
    }
   ],
   "source": [
    "# Required installations for transformers and datasets\n",
    "!pip install transformers datasets\n",
    "!pip install keras huggingface_hub\n",
    "!pip install tensorflow\n",
    "!pip install python-dotenv\n",
    "!pip install zstandard\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer activations - LLaMA 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/drew99/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Hugging Face Login (get token from os environment)\n",
    "# login(os.getenv(\"HF_ACCESS_TOKEN\"))\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "\n",
    "# Could also later use \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc88d87534b84095b6da0e281e32b9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the LLaMA 3.2B model without quantization (for now)\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device) # 16-bit precision\n",
    "\n",
    "# Some other LLaMA models to try:\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B\"\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\n",
    "# https://huggingface.co/mradermacher/Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO-i1-GGUF  i1-Q4_K_M\n",
    "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ\n",
    "\n",
    "# Prepare 4-bit quantization configuration (optional)\n",
    "# Uncomment the following lines if you wish to use quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (1): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (2): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (3): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (4): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (5): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (6): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (7): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (8): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (9): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (10): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (11): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (12): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (13): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (14): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (15): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (16): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (17): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (18): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (19): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (20): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (21): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (22): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (23): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (24): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (25): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (26): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (27): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers) # Specific to LLaMA model # https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) \n",
    "# for other models, you may need to use model.transformer.h[15] instead of model.model.layers[15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual stream shape at layer 16: (1, 6, 3072)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example input\n",
    "input_text = \"Your input text here.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Access the residual stream activation\n",
    "residual_stream = activation_cache[0]\n",
    "print(f\"Residual stream shape at layer {layer_index+1}: {residual_stream.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716272d6ac694484bf96048b080049d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset and saving activations in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11006/2903579871.py:55: RuntimeWarning: overflow encountered in cast\n",
      "  batch_activations = np.hstack((batch_activations, sent_idx, token_idx, tokens)).astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data shape: (2910, 3075)\n",
      "all_data shape: (5254, 3075)\n",
      "all_data shape: (7698, 3075)\n",
      "all_data shape: (10530, 3075)\n",
      "all_data shape: (13452, 3075)\n",
      "all_data shape: (16771, 3075)\n",
      "all_data shape: (19101, 3075)\n",
      "all_data shape: (21961, 3075)\n",
      "all_data shape: (24559, 3075)\n",
      "all_data shape: (27488, 3075)\n",
      "all_data shape: (30681, 3075)\n",
      "all_data shape: (32968, 3075)\n",
      "all_data shape: (35208, 3075)\n",
      "all_data shape: (37700, 3075)\n",
      "all_data shape: (40361, 3075)\n",
      "all_data shape: (43586, 3075)\n",
      "all_data shape: (46561, 3075)\n",
      "all_data shape: (49329, 3075)\n",
      "all_data shape: (52517, 3075)\n",
      "all_data shape: (55162, 3075)\n",
      "all_data shape: (58179, 3075)\n",
      "all_data shape: (61216, 3075)\n",
      "all_data shape: (64546, 3075)\n",
      "all_data shape: (67435, 3075)\n",
      "all_data shape: (70601, 3075)\n",
      "all_data shape: (73867, 3075)\n",
      "all_data shape: (76587, 3075)\n",
      "all_data shape: (79081, 3075)\n",
      "all_data shape: (81939, 3075)\n",
      "Saved file 1 == 81920 examples\n",
      "all_data shape: (2555, 3075)\n",
      "all_data shape: (5696, 3075)\n",
      "all_data shape: (8355, 3075)\n",
      "all_data shape: (10896, 3075)\n",
      "all_data shape: (14261, 3075)\n",
      "all_data shape: (17119, 3075)\n",
      "all_data shape: (19211, 3075)\n",
      "all_data shape: (22187, 3075)\n",
      "all_data shape: (24274, 3075)\n",
      "all_data shape: (27584, 3075)\n",
      "all_data shape: (30584, 3075)\n",
      "all_data shape: (33554, 3075)\n",
      "all_data shape: (35760, 3075)\n",
      "all_data shape: (38183, 3075)\n",
      "all_data shape: (41165, 3075)\n",
      "all_data shape: (43734, 3075)\n",
      "all_data shape: (46274, 3075)\n",
      "all_data shape: (49317, 3075)\n",
      "all_data shape: (52241, 3075)\n",
      "all_data shape: (54818, 3075)\n",
      "all_data shape: (57043, 3075)\n",
      "all_data shape: (59749, 3075)\n",
      "all_data shape: (62916, 3075)\n",
      "all_data shape: (65123, 3075)\n",
      "all_data shape: (67729, 3075)\n",
      "all_data shape: (70168, 3075)\n",
      "all_data shape: (72606, 3075)\n",
      "all_data shape: (75496, 3075)\n",
      "all_data shape: (78573, 3075)\n",
      "all_data shape: (81117, 3075)\n",
      "all_data shape: (84459, 3075)\n",
      "Saved file 2 == 163840 examples\n",
      "all_data shape: (4582, 3075)\n",
      "all_data shape: (8021, 3075)\n",
      "all_data shape: (10730, 3075)\n",
      "all_data shape: (13031, 3075)\n",
      "all_data shape: (16145, 3075)\n",
      "all_data shape: (18590, 3075)\n",
      "all_data shape: (21637, 3075)\n",
      "all_data shape: (24140, 3075)\n",
      "all_data shape: (26609, 3075)\n",
      "all_data shape: (29472, 3075)\n",
      "all_data shape: (32280, 3075)\n",
      "all_data shape: (35248, 3075)\n",
      "all_data shape: (38613, 3075)\n",
      "all_data shape: (41151, 3075)\n",
      "all_data shape: (44323, 3075)\n",
      "all_data shape: (47990, 3075)\n",
      "all_data shape: (49925, 3075)\n",
      "all_data shape: (52386, 3075)\n",
      "all_data shape: (54911, 3075)\n",
      "all_data shape: (57994, 3075)\n",
      "all_data shape: (60925, 3075)\n",
      "all_data shape: (63831, 3075)\n",
      "all_data shape: (67003, 3075)\n",
      "all_data shape: (70006, 3075)\n",
      "all_data shape: (72849, 3075)\n",
      "all_data shape: (75952, 3075)\n",
      "all_data shape: (78606, 3075)\n",
      "all_data shape: (81146, 3075)\n",
      "all_data shape: (84311, 3075)\n",
      "Saved file 3 == 245760 examples\n",
      "all_data shape: (5300, 3075)\n",
      "all_data shape: (7777, 3075)\n",
      "all_data shape: (10464, 3075)\n",
      "all_data shape: (13107, 3075)\n",
      "all_data shape: (16235, 3075)\n",
      "all_data shape: (19037, 3075)\n",
      "all_data shape: (21421, 3075)\n",
      "all_data shape: (24470, 3075)\n",
      "all_data shape: (27406, 3075)\n",
      "all_data shape: (29741, 3075)\n",
      "all_data shape: (32304, 3075)\n",
      "all_data shape: (34348, 3075)\n",
      "all_data shape: (37092, 3075)\n",
      "all_data shape: (39384, 3075)\n",
      "all_data shape: (42400, 3075)\n",
      "all_data shape: (45857, 3075)\n",
      "all_data shape: (48333, 3075)\n",
      "all_data shape: (50990, 3075)\n",
      "all_data shape: (52882, 3075)\n",
      "all_data shape: (55541, 3075)\n",
      "all_data shape: (58274, 3075)\n",
      "all_data shape: (61272, 3075)\n",
      "all_data shape: (63695, 3075)\n",
      "all_data shape: (66199, 3075)\n",
      "all_data shape: (68213, 3075)\n",
      "all_data shape: (71616, 3075)\n",
      "all_data shape: (73758, 3075)\n",
      "all_data shape: (76332, 3075)\n",
      "all_data shape: (79315, 3075)\n",
      "all_data shape: (81502, 3075)\n",
      "all_data shape: (84158, 3075)\n",
      "Saved file 4 == 327680 examples\n",
      "all_data shape: (3856, 3075)\n",
      "all_data shape: (6284, 3075)\n",
      "all_data shape: (8761, 3075)\n",
      "all_data shape: (11554, 3075)\n",
      "all_data shape: (13855, 3075)\n",
      "all_data shape: (16974, 3075)\n",
      "all_data shape: (19971, 3075)\n",
      "all_data shape: (23683, 3075)\n",
      "all_data shape: (27018, 3075)\n",
      "all_data shape: (29989, 3075)\n",
      "all_data shape: (32257, 3075)\n",
      "all_data shape: (34340, 3075)\n",
      "all_data shape: (37511, 3075)\n",
      "all_data shape: (40592, 3075)\n",
      "all_data shape: (44225, 3075)\n",
      "all_data shape: (47185, 3075)\n",
      "all_data shape: (49666, 3075)\n",
      "all_data shape: (52419, 3075)\n",
      "all_data shape: (55652, 3075)\n",
      "all_data shape: (58304, 3075)\n",
      "all_data shape: (60566, 3075)\n",
      "all_data shape: (63608, 3075)\n",
      "all_data shape: (66476, 3075)\n",
      "all_data shape: (69363, 3075)\n",
      "all_data shape: (72624, 3075)\n",
      "all_data shape: (75772, 3075)\n",
      "all_data shape: (78233, 3075)\n",
      "all_data shape: (82063, 3075)\n",
      "Saved file 5 == 409600 examples\n",
      "all_data shape: (3239, 3075)\n",
      "all_data shape: (5833, 3075)\n",
      "all_data shape: (8346, 3075)\n",
      "all_data shape: (10895, 3075)\n",
      "all_data shape: (13597, 3075)\n",
      "all_data shape: (16655, 3075)\n",
      "all_data shape: (19175, 3075)\n",
      "all_data shape: (21990, 3075)\n",
      "all_data shape: (25023, 3075)\n",
      "all_data shape: (28446, 3075)\n",
      "all_data shape: (31336, 3075)\n",
      "all_data shape: (34046, 3075)\n",
      "all_data shape: (37092, 3075)\n",
      "all_data shape: (39488, 3075)\n",
      "all_data shape: (42899, 3075)\n",
      "all_data shape: (46124, 3075)\n",
      "all_data shape: (48360, 3075)\n",
      "all_data shape: (51732, 3075)\n",
      "all_data shape: (55137, 3075)\n",
      "all_data shape: (57418, 3075)\n",
      "all_data shape: (59491, 3075)\n",
      "all_data shape: (62880, 3075)\n",
      "all_data shape: (66343, 3075)\n",
      "all_data shape: (69679, 3075)\n",
      "all_data shape: (72203, 3075)\n",
      "all_data shape: (74774, 3075)\n",
      "all_data shape: (76880, 3075)\n",
      "all_data shape: (80179, 3075)\n",
      "all_data shape: (83234, 3075)\n",
      "Saved file 6 == 491520 examples\n",
      "all_data shape: (4278, 3075)\n",
      "all_data shape: (7376, 3075)\n",
      "all_data shape: (9955, 3075)\n",
      "all_data shape: (13648, 3075)\n",
      "all_data shape: (16870, 3075)\n",
      "all_data shape: (19569, 3075)\n",
      "all_data shape: (22521, 3075)\n",
      "all_data shape: (25567, 3075)\n",
      "all_data shape: (28118, 3075)\n",
      "all_data shape: (30423, 3075)\n",
      "all_data shape: (32770, 3075)\n",
      "all_data shape: (34812, 3075)\n",
      "all_data shape: (37711, 3075)\n",
      "all_data shape: (40429, 3075)\n",
      "all_data shape: (43985, 3075)\n",
      "all_data shape: (47434, 3075)\n",
      "all_data shape: (50499, 3075)\n",
      "all_data shape: (52961, 3075)\n",
      "all_data shape: (56038, 3075)\n",
      "all_data shape: (58919, 3075)\n",
      "all_data shape: (61354, 3075)\n",
      "all_data shape: (64083, 3075)\n",
      "all_data shape: (67354, 3075)\n",
      "all_data shape: (70855, 3075)\n",
      "all_data shape: (73961, 3075)\n",
      "all_data shape: (76721, 3075)\n",
      "all_data shape: (79709, 3075)\n",
      "all_data shape: (82403, 3075)\n",
      "Saved file 7 == 573440 examples\n",
      "all_data shape: (3559, 3075)\n",
      "all_data shape: (5941, 3075)\n",
      "all_data shape: (8794, 3075)\n",
      "all_data shape: (11276, 3075)\n",
      "all_data shape: (13894, 3075)\n",
      "all_data shape: (16970, 3075)\n",
      "all_data shape: (20376, 3075)\n",
      "all_data shape: (23299, 3075)\n",
      "all_data shape: (26400, 3075)\n",
      "all_data shape: (29633, 3075)\n",
      "all_data shape: (31949, 3075)\n",
      "all_data shape: (34367, 3075)\n",
      "all_data shape: (37639, 3075)\n",
      "all_data shape: (40030, 3075)\n",
      "all_data shape: (43353, 3075)\n",
      "all_data shape: (46002, 3075)\n",
      "all_data shape: (48820, 3075)\n",
      "all_data shape: (50465, 3075)\n",
      "all_data shape: (52874, 3075)\n",
      "all_data shape: (56143, 3075)\n",
      "all_data shape: (59715, 3075)\n",
      "all_data shape: (61691, 3075)\n",
      "all_data shape: (64424, 3075)\n",
      "all_data shape: (66717, 3075)\n",
      "all_data shape: (69514, 3075)\n",
      "all_data shape: (72921, 3075)\n",
      "all_data shape: (75934, 3075)\n",
      "all_data shape: (78786, 3075)\n",
      "all_data shape: (80839, 3075)\n",
      "all_data shape: (84346, 3075)\n",
      "Saved file 8 == 655360 examples\n",
      "all_data shape: (4995, 3075)\n",
      "all_data shape: (8565, 3075)\n",
      "all_data shape: (11409, 3075)\n",
      "all_data shape: (14610, 3075)\n",
      "all_data shape: (17145, 3075)\n",
      "all_data shape: (19923, 3075)\n",
      "all_data shape: (21822, 3075)\n",
      "all_data shape: (24111, 3075)\n",
      "all_data shape: (26809, 3075)\n",
      "all_data shape: (29551, 3075)\n",
      "all_data shape: (32074, 3075)\n",
      "all_data shape: (35241, 3075)\n",
      "all_data shape: (38299, 3075)\n",
      "all_data shape: (40535, 3075)\n",
      "all_data shape: (43971, 3075)\n",
      "all_data shape: (45662, 3075)\n",
      "all_data shape: (48868, 3075)\n",
      "all_data shape: (51564, 3075)\n",
      "all_data shape: (54924, 3075)\n",
      "all_data shape: (58550, 3075)\n",
      "all_data shape: (61707, 3075)\n",
      "all_data shape: (64749, 3075)\n",
      "all_data shape: (67536, 3075)\n",
      "all_data shape: (70411, 3075)\n",
      "all_data shape: (73198, 3075)\n",
      "all_data shape: (75101, 3075)\n",
      "all_data shape: (77916, 3075)\n",
      "all_data shape: (80417, 3075)\n",
      "all_data shape: (82955, 3075)\n",
      "Saved file 9 == 737280 examples\n",
      "all_data shape: (4128, 3075)\n",
      "all_data shape: (7636, 3075)\n",
      "all_data shape: (10614, 3075)\n",
      "all_data shape: (13443, 3075)\n",
      "all_data shape: (16447, 3075)\n",
      "all_data shape: (20073, 3075)\n",
      "all_data shape: (22751, 3075)\n",
      "all_data shape: (25222, 3075)\n",
      "all_data shape: (27765, 3075)\n",
      "all_data shape: (30859, 3075)\n",
      "all_data shape: (33424, 3075)\n",
      "all_data shape: (36253, 3075)\n",
      "all_data shape: (38900, 3075)\n",
      "all_data shape: (42124, 3075)\n",
      "all_data shape: (45723, 3075)\n",
      "all_data shape: (48108, 3075)\n",
      "all_data shape: (50667, 3075)\n",
      "all_data shape: (53237, 3075)\n",
      "all_data shape: (56594, 3075)\n",
      "all_data shape: (59002, 3075)\n",
      "all_data shape: (61477, 3075)\n",
      "all_data shape: (64622, 3075)\n",
      "all_data shape: (67393, 3075)\n",
      "all_data shape: (70037, 3075)\n",
      "all_data shape: (73204, 3075)\n",
      "all_data shape: (76235, 3075)\n",
      "all_data shape: (78449, 3075)\n",
      "all_data shape: (81337, 3075)\n",
      "all_data shape: (83959, 3075)\n",
      "Saved file 10 == 819200 examples\n",
      "all_data shape: (4682, 3075)\n",
      "all_data shape: (7559, 3075)\n",
      "all_data shape: (10394, 3075)\n",
      "all_data shape: (13379, 3075)\n",
      "all_data shape: (15999, 3075)\n",
      "all_data shape: (18032, 3075)\n",
      "all_data shape: (21283, 3075)\n",
      "all_data shape: (24291, 3075)\n",
      "all_data shape: (27235, 3075)\n",
      "all_data shape: (29974, 3075)\n",
      "all_data shape: (32822, 3075)\n",
      "all_data shape: (36565, 3075)\n",
      "all_data shape: (39666, 3075)\n",
      "all_data shape: (42189, 3075)\n",
      "all_data shape: (45082, 3075)\n",
      "all_data shape: (48431, 3075)\n",
      "all_data shape: (50839, 3075)\n",
      "all_data shape: (54138, 3075)\n",
      "all_data shape: (57696, 3075)\n",
      "all_data shape: (60471, 3075)\n",
      "all_data shape: (62923, 3075)\n",
      "all_data shape: (65891, 3075)\n",
      "all_data shape: (68739, 3075)\n",
      "all_data shape: (72137, 3075)\n",
      "all_data shape: (75699, 3075)\n",
      "all_data shape: (78647, 3075)\n",
      "all_data shape: (81633, 3075)\n",
      "all_data shape: (84058, 3075)\n",
      "Saved file 11 == 901120 examples\n",
      "all_data shape: (4358, 3075)\n",
      "all_data shape: (7785, 3075)\n",
      "all_data shape: (10812, 3075)\n",
      "all_data shape: (13296, 3075)\n",
      "all_data shape: (16290, 3075)\n",
      "all_data shape: (18893, 3075)\n",
      "all_data shape: (21017, 3075)\n",
      "all_data shape: (23397, 3075)\n",
      "all_data shape: (26261, 3075)\n",
      "all_data shape: (28239, 3075)\n",
      "all_data shape: (30482, 3075)\n",
      "all_data shape: (33376, 3075)\n",
      "all_data shape: (36790, 3075)\n",
      "all_data shape: (38497, 3075)\n",
      "all_data shape: (40877, 3075)\n",
      "all_data shape: (43906, 3075)\n",
      "all_data shape: (46625, 3075)\n",
      "all_data shape: (48366, 3075)\n",
      "all_data shape: (51239, 3075)\n",
      "all_data shape: (54266, 3075)\n",
      "all_data shape: (57201, 3075)\n",
      "all_data shape: (60066, 3075)\n",
      "all_data shape: (62062, 3075)\n",
      "all_data shape: (65202, 3075)\n",
      "all_data shape: (67802, 3075)\n",
      "all_data shape: (70308, 3075)\n",
      "all_data shape: (73871, 3075)\n",
      "all_data shape: (76777, 3075)\n",
      "all_data shape: (79532, 3075)\n",
      "all_data shape: (81929, 3075)\n",
      "Saved file 12 == 983040 examples\n",
      "all_data shape: (3142, 3075)\n",
      "all_data shape: (6320, 3075)\n",
      "all_data shape: (9686, 3075)\n",
      "all_data shape: (12885, 3075)\n",
      "all_data shape: (15930, 3075)\n",
      "all_data shape: (18394, 3075)\n",
      "all_data shape: (21465, 3075)\n",
      "all_data shape: (24493, 3075)\n",
      "all_data shape: (27663, 3075)\n",
      "all_data shape: (30553, 3075)\n",
      "all_data shape: (33467, 3075)\n",
      "all_data shape: (36096, 3075)\n",
      "all_data shape: (38555, 3075)\n",
      "all_data shape: (41196, 3075)\n",
      "all_data shape: (44458, 3075)\n",
      "all_data shape: (46309, 3075)\n",
      "all_data shape: (49607, 3075)\n",
      "all_data shape: (51875, 3075)\n",
      "all_data shape: (55219, 3075)\n",
      "all_data shape: (57892, 3075)\n",
      "all_data shape: (60109, 3075)\n",
      "all_data shape: (63910, 3075)\n",
      "all_data shape: (67757, 3075)\n",
      "all_data shape: (70650, 3075)\n",
      "all_data shape: (73236, 3075)\n",
      "all_data shape: (76271, 3075)\n",
      "all_data shape: (79897, 3075)\n",
      "all_data shape: (82743, 3075)\n",
      "Saved file 13 == 1064960 examples\n",
      "all_data shape: (3723, 3075)\n",
      "all_data shape: (5962, 3075)\n",
      "all_data shape: (9485, 3075)\n",
      "all_data shape: (12085, 3075)\n",
      "all_data shape: (14425, 3075)\n",
      "all_data shape: (17620, 3075)\n",
      "all_data shape: (20751, 3075)\n",
      "all_data shape: (23619, 3075)\n",
      "all_data shape: (25335, 3075)\n",
      "all_data shape: (27447, 3075)\n",
      "all_data shape: (30684, 3075)\n",
      "all_data shape: (33534, 3075)\n",
      "all_data shape: (36835, 3075)\n",
      "all_data shape: (40454, 3075)\n",
      "all_data shape: (43094, 3075)\n",
      "all_data shape: (46378, 3075)\n",
      "all_data shape: (48819, 3075)\n",
      "all_data shape: (52108, 3075)\n",
      "all_data shape: (54767, 3075)\n",
      "all_data shape: (57943, 3075)\n",
      "all_data shape: (60761, 3075)\n",
      "all_data shape: (64305, 3075)\n",
      "all_data shape: (66661, 3075)\n",
      "all_data shape: (69905, 3075)\n",
      "all_data shape: (72685, 3075)\n",
      "all_data shape: (75339, 3075)\n",
      "all_data shape: (78032, 3075)\n",
      "all_data shape: (81388, 3075)\n",
      "all_data shape: (84413, 3075)\n",
      "Saved file 14 == 1146880 examples\n",
      "all_data shape: (5343, 3075)\n",
      "all_data shape: (8437, 3075)\n",
      "all_data shape: (11249, 3075)\n",
      "all_data shape: (13364, 3075)\n",
      "all_data shape: (16583, 3075)\n",
      "all_data shape: (19144, 3075)\n",
      "all_data shape: (22106, 3075)\n",
      "all_data shape: (24776, 3075)\n",
      "all_data shape: (27315, 3075)\n",
      "all_data shape: (29602, 3075)\n",
      "all_data shape: (33111, 3075)\n",
      "all_data shape: (36533, 3075)\n",
      "all_data shape: (39243, 3075)\n",
      "all_data shape: (42612, 3075)\n",
      "all_data shape: (45155, 3075)\n",
      "all_data shape: (47176, 3075)\n",
      "all_data shape: (50240, 3075)\n",
      "all_data shape: (53197, 3075)\n",
      "all_data shape: (56742, 3075)\n",
      "all_data shape: (59636, 3075)\n",
      "all_data shape: (62616, 3075)\n",
      "all_data shape: (65334, 3075)\n",
      "all_data shape: (68666, 3075)\n",
      "all_data shape: (72012, 3075)\n",
      "all_data shape: (75387, 3075)\n",
      "all_data shape: (78242, 3075)\n",
      "all_data shape: (81013, 3075)\n",
      "all_data shape: (84285, 3075)\n",
      "Saved file 15 == 1228800 examples\n",
      "all_data shape: (5488, 3075)\n",
      "all_data shape: (8694, 3075)\n",
      "all_data shape: (11057, 3075)\n",
      "all_data shape: (14264, 3075)\n",
      "all_data shape: (17082, 3075)\n",
      "all_data shape: (19490, 3075)\n",
      "all_data shape: (23125, 3075)\n",
      "all_data shape: (25609, 3075)\n",
      "all_data shape: (28431, 3075)\n",
      "all_data shape: (31788, 3075)\n",
      "all_data shape: (34849, 3075)\n",
      "all_data shape: (36999, 3075)\n",
      "all_data shape: (39989, 3075)\n",
      "all_data shape: (42808, 3075)\n",
      "all_data shape: (45495, 3075)\n",
      "all_data shape: (48038, 3075)\n",
      "all_data shape: (50066, 3075)\n",
      "all_data shape: (53112, 3075)\n",
      "all_data shape: (55980, 3075)\n",
      "all_data shape: (57736, 3075)\n",
      "all_data shape: (59979, 3075)\n",
      "all_data shape: (62907, 3075)\n",
      "all_data shape: (65925, 3075)\n",
      "all_data shape: (68816, 3075)\n",
      "all_data shape: (71653, 3075)\n",
      "all_data shape: (74636, 3075)\n",
      "all_data shape: (77396, 3075)\n",
      "all_data shape: (79530, 3075)\n",
      "all_data shape: (82289, 3075)\n",
      "Saved file 16 == 1310720 examples\n",
      "all_data shape: (3283, 3075)\n",
      "all_data shape: (6291, 3075)\n",
      "all_data shape: (9008, 3075)\n",
      "all_data shape: (12033, 3075)\n",
      "all_data shape: (14736, 3075)\n",
      "all_data shape: (18035, 3075)\n",
      "all_data shape: (20770, 3075)\n",
      "all_data shape: (23603, 3075)\n",
      "all_data shape: (26312, 3075)\n",
      "all_data shape: (29397, 3075)\n",
      "all_data shape: (32767, 3075)\n",
      "all_data shape: (36002, 3075)\n",
      "all_data shape: (38781, 3075)\n",
      "all_data shape: (41730, 3075)\n",
      "all_data shape: (44612, 3075)\n",
      "all_data shape: (48209, 3075)\n",
      "all_data shape: (50605, 3075)\n",
      "all_data shape: (53429, 3075)\n",
      "all_data shape: (57254, 3075)\n",
      "all_data shape: (59500, 3075)\n",
      "all_data shape: (62432, 3075)\n",
      "all_data shape: (65226, 3075)\n",
      "all_data shape: (68063, 3075)\n",
      "all_data shape: (70869, 3075)\n",
      "all_data shape: (73562, 3075)\n",
      "all_data shape: (76314, 3075)\n",
      "all_data shape: (78924, 3075)\n",
      "all_data shape: (81952, 3075)\n",
      "Saved file 17 == 1392640 examples\n",
      "all_data shape: (2858, 3075)\n",
      "all_data shape: (5672, 3075)\n",
      "all_data shape: (8617, 3075)\n",
      "all_data shape: (10354, 3075)\n",
      "all_data shape: (13088, 3075)\n",
      "all_data shape: (16552, 3075)\n",
      "all_data shape: (19511, 3075)\n",
      "all_data shape: (22348, 3075)\n",
      "all_data shape: (25160, 3075)\n",
      "all_data shape: (27802, 3075)\n",
      "all_data shape: (31149, 3075)\n",
      "all_data shape: (34305, 3075)\n",
      "all_data shape: (36813, 3075)\n",
      "all_data shape: (39533, 3075)\n",
      "all_data shape: (42694, 3075)\n",
      "all_data shape: (45354, 3075)\n",
      "all_data shape: (48230, 3075)\n",
      "all_data shape: (50757, 3075)\n",
      "all_data shape: (53601, 3075)\n",
      "all_data shape: (56499, 3075)\n",
      "all_data shape: (59186, 3075)\n",
      "all_data shape: (61963, 3075)\n",
      "all_data shape: (64439, 3075)\n",
      "all_data shape: (66954, 3075)\n",
      "all_data shape: (69721, 3075)\n",
      "all_data shape: (72625, 3075)\n",
      "all_data shape: (75666, 3075)\n",
      "all_data shape: (78596, 3075)\n",
      "all_data shape: (81501, 3075)\n",
      "all_data shape: (84106, 3075)\n",
      "Saved file 18 == 1474560 examples\n",
      "all_data shape: (5734, 3075)\n",
      "all_data shape: (9450, 3075)\n",
      "all_data shape: (12939, 3075)\n",
      "all_data shape: (15598, 3075)\n",
      "all_data shape: (18393, 3075)\n",
      "all_data shape: (21123, 3075)\n",
      "all_data shape: (23856, 3075)\n",
      "all_data shape: (27357, 3075)\n",
      "all_data shape: (30841, 3075)\n",
      "all_data shape: (33648, 3075)\n",
      "all_data shape: (36544, 3075)\n",
      "all_data shape: (38549, 3075)\n",
      "all_data shape: (41499, 3075)\n",
      "all_data shape: (44280, 3075)\n",
      "all_data shape: (47035, 3075)\n",
      "all_data shape: (50552, 3075)\n",
      "all_data shape: (53824, 3075)\n",
      "all_data shape: (56632, 3075)\n",
      "all_data shape: (59670, 3075)\n",
      "all_data shape: (62641, 3075)\n",
      "all_data shape: (65885, 3075)\n",
      "all_data shape: (69410, 3075)\n",
      "all_data shape: (72468, 3075)\n",
      "all_data shape: (75106, 3075)\n",
      "all_data shape: (77420, 3075)\n",
      "all_data shape: (80434, 3075)\n",
      "all_data shape: (83099, 3075)\n",
      "Saved file 19 == 1556480 examples\n",
      "all_data shape: (3383, 3075)\n",
      "all_data shape: (5879, 3075)\n",
      "all_data shape: (7823, 3075)\n",
      "all_data shape: (10011, 3075)\n",
      "all_data shape: (13083, 3075)\n",
      "all_data shape: (16176, 3075)\n",
      "all_data shape: (18484, 3075)\n",
      "all_data shape: (21987, 3075)\n",
      "all_data shape: (24550, 3075)\n",
      "all_data shape: (27711, 3075)\n",
      "all_data shape: (30296, 3075)\n",
      "all_data shape: (33093, 3075)\n",
      "all_data shape: (36825, 3075)\n",
      "all_data shape: (39338, 3075)\n",
      "all_data shape: (42146, 3075)\n",
      "all_data shape: (44953, 3075)\n",
      "all_data shape: (48169, 3075)\n",
      "all_data shape: (51309, 3075)\n",
      "all_data shape: (54321, 3075)\n",
      "all_data shape: (57228, 3075)\n",
      "all_data shape: (60035, 3075)\n",
      "all_data shape: (62311, 3075)\n",
      "all_data shape: (65305, 3075)\n",
      "all_data shape: (68506, 3075)\n",
      "all_data shape: (70724, 3075)\n",
      "all_data shape: (73994, 3075)\n",
      "all_data shape: (77076, 3075)\n",
      "all_data shape: (80218, 3075)\n",
      "all_data shape: (83758, 3075)\n",
      "Saved file 20 == 1638400 examples\n",
      "all_data shape: (4635, 3075)\n",
      "all_data shape: (7764, 3075)\n",
      "all_data shape: (11311, 3075)\n",
      "all_data shape: (14356, 3075)\n",
      "all_data shape: (16827, 3075)\n",
      "all_data shape: (19480, 3075)\n",
      "all_data shape: (21914, 3075)\n",
      "all_data shape: (24739, 3075)\n",
      "all_data shape: (27564, 3075)\n",
      "all_data shape: (30106, 3075)\n",
      "all_data shape: (32176, 3075)\n",
      "all_data shape: (35913, 3075)\n",
      "all_data shape: (38474, 3075)\n",
      "all_data shape: (41716, 3075)\n",
      "all_data shape: (44932, 3075)\n",
      "all_data shape: (47981, 3075)\n",
      "all_data shape: (50931, 3075)\n",
      "all_data shape: (53399, 3075)\n",
      "all_data shape: (55898, 3075)\n",
      "all_data shape: (58441, 3075)\n",
      "all_data shape: (61472, 3075)\n",
      "all_data shape: (64679, 3075)\n",
      "all_data shape: (67058, 3075)\n",
      "all_data shape: (70367, 3075)\n",
      "all_data shape: (72969, 3075)\n",
      "all_data shape: (76146, 3075)\n",
      "all_data shape: (79227, 3075)\n",
      "all_data shape: (81475, 3075)\n",
      "all_data shape: (84112, 3075)\n",
      "Saved file 21 == 1720320 examples\n",
      "all_data shape: (5002, 3075)\n",
      "all_data shape: (7529, 3075)\n",
      "all_data shape: (9861, 3075)\n",
      "all_data shape: (12088, 3075)\n",
      "all_data shape: (14942, 3075)\n",
      "all_data shape: (17329, 3075)\n",
      "all_data shape: (20373, 3075)\n",
      "all_data shape: (23794, 3075)\n",
      "all_data shape: (26744, 3075)\n",
      "all_data shape: (30429, 3075)\n",
      "all_data shape: (33450, 3075)\n",
      "all_data shape: (36666, 3075)\n",
      "all_data shape: (40157, 3075)\n",
      "all_data shape: (42862, 3075)\n",
      "all_data shape: (45861, 3075)\n",
      "all_data shape: (48728, 3075)\n",
      "all_data shape: (51272, 3075)\n",
      "Finished processing and saving all batches\n"
     ]
    }
   ],
   "source": [
    "# Load the first 30 million examples from 'The Pile' dataset \n",
    "# https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "# data_len = 30_000_000\n",
    "data_len = 10_000\n",
    "# split_str = f\"train[:{data_len}]\"\n",
    "dataset = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
    "\n",
    "# Set up processing parameters\n",
    "os.makedirs(\"activations_data\", exist_ok=True)\n",
    "\n",
    "# Initialize accumulators and parameters\n",
    "batch_size = 8 # number of sentences in a batch\n",
    "file_size = 10*8192 # number of examples in a file\n",
    "files_saved = 0\n",
    "batch_texts = []\n",
    "activation_cache = [] # cache of activations for a batch\n",
    "all_data = np.empty((0, 3075), dtype=np.float16)  # 3072 + 3 (sent_idx, token_idx, token)\n",
    "\n",
    "# Create batches from the dataset\n",
    "print(\"Processing dataset and saving activations in batches...\")\n",
    "for i, example in enumerate(dataset):\n",
    "    batch_texts.append(example['text'])\n",
    "    \n",
    "    if (i + 1) % batch_size == 0 or i + 1 >= data_len:\n",
    "        # Process full batch or final partial batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "        \n",
    "        # Convert activation_cache to numpy array and reshape\n",
    "        batch_activations = np.array(activation_cache)\n",
    "        \n",
    "        # Reshape batch_activations from (1, 8, 42, 3072) to (8*42, 3072)\n",
    "        batch_activations = batch_activations.reshape(batch_activations.shape[1] * batch_activations.shape[2], -1)\n",
    "\n",
    "        # Create sentence index array (sent_idx) and token index array (token_idx)\n",
    "        # sent_idx = [1 1 1 1 1; 2 2 2 2 2; 3 3 3 3 3; ...]\n",
    "        # token_idx = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; ...]\n",
    "        num_sentences, num_tokens = inputs['attention_mask'].shape # (8, 42)\n",
    "        sent_idx = np.repeat(np.arange(1, num_sentences + 1), num_tokens).reshape(-1, 1)  # Shape: (8*42, 1)\n",
    "        sent_idx = sent_idx + (i - batch_size) # offset by batch index\n",
    "        token_idx = np.tile(np.arange(1, num_tokens + 1), num_sentences).reshape(-1, 1)    # Shape: (8*42, 1)\n",
    "        token_idx = token_idx - 1 # offset by 1\n",
    "        # also save tokens id from tokenizer\n",
    "        tokens = inputs['input_ids'].cpu().numpy().reshape(-1, 1)\n",
    "                \n",
    "        # Stack activations, sent_idx, and token_idx\n",
    "        batch_activations = np.hstack((batch_activations, sent_idx, token_idx, tokens)).astype(np.float16)\n",
    "\n",
    "        # Remove rows where attention mask is 0\n",
    "        attention_mask = inputs['attention_mask'].cpu().numpy().reshape(-1)\n",
    "        batch_activations = batch_activations[attention_mask != 0]\n",
    "\n",
    "        # Stack to all_data\n",
    "        all_data = np.vstack((all_data, batch_activations))\n",
    "        print(f\"all_data shape: {all_data.shape}\")\n",
    "\n",
    "        # Save to file if file_size limit is reached\n",
    "        if all_data.shape[0] >= file_size:\n",
    "            data_to_save = all_data[:file_size, :]\n",
    "            np.save(f\"activations_data/activations_batch_{files_saved}.npy\", data_to_save)\n",
    "            files_saved += 1\n",
    "            print(f\"Saved file {files_saved} == {file_size*files_saved} examples\")\n",
    "            all_data = all_data[file_size:, :]  # Retain any remaining rows\n",
    "            \n",
    "        # Reset for next batch\n",
    "        batch_texts = []\n",
    "        activation_cache = []\n",
    "\n",
    "    if i + 1 >= data_len:\n",
    "        break\n",
    "\n",
    "# Save any remaining data\n",
    "if all_data.shape[0] > 0:\n",
    "    np.save(f\"activations_data/activations_batch_{files_saved}.npy\", all_data)\n",
    "    del all_data\n",
    "\n",
    "print(\"Finished processing and saving all batches\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process 1M examples:\n",
      "14.236111111111112 days\n",
      "Memory to store 1M examples:\n",
      "258.048 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Time to process 1M examples:\")\n",
    "# 20.5 seconds for 10 batches of 100 examples\n",
    "# how much time to process 1M examples? = 14.2 days\n",
    "print(20.5 / 1000 * 1e6 / 60 / 24, \"days\") # in days\n",
    "\n",
    "print(\"Memory to store 1M examples:\")\n",
    "# each batch is 0.0258048 GB\n",
    "# how much memory to store 1M examples? = 258 GB\n",
    "print(0.0258048 / 100 * 1e6, \"GB\") # in GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted and GPU memory freed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Unregister hook\n",
    "hook_handle.remove()\n",
    "# Shutdown the model use del and free gpu\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model deleted and GPU memory freed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3466077266.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    brek here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "brek here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = np.load(\"activations_data/activations_batch_0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81920, 3075)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.shape # of float16 = 2 bytes per element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float16')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activations type\n",
    "activations.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.503808"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.nbytes / 1e9 # in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12902400/3072 # = 100 examples of 42 token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1572864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*3072 # = 1.5M features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded, encoded\n",
    "\n",
    "# Initialize model parameters\n",
    "input_dim = 3072  \n",
    "hidden_dim = 2 ** 9  # Adjust based on your requirements\n",
    "\n",
    "# Initialize the model\n",
    "model = SparseAutoencoder(input_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, data_dir, batch_size, train):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.file_names = sorted([f for f in os.listdir(data_dir) if f.endswith('.npy') and f.startswith('activations_batch')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_names[idx])\n",
    "        activations = np.load(file_path)\n",
    "        if not self.train: # if test\n",
    "            sent_idx = activations[:, -3]\n",
    "            token_idx = activations[:, -2] \n",
    "            token = activations[:, -1]\n",
    "        # remove last 3 columns (sent_idx, token_idx, and token)\n",
    "        activations = activations[:, :-3]\n",
    "        # normalize activations to have mean 0 and std 1\n",
    "        activations = (activations - np.mean(activations)) / np.std(activations)\n",
    "        if self.train:\n",
    "            # random subsample 8192 examples\n",
    "            activations = activations[np.random.choice(activations.shape[0], self.batch_size, replace=False), :]\n",
    "        else: # test\n",
    "            return torch.tensor(activations, dtype=torch.float32), sent_idx, token_idx, token\n",
    "        return torch.tensor(activations, dtype=torch.float32)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "data_dir = \"activations_data\"\n",
    "dataset = ActivationDataset(data_dir, batch_size=8192, train=True) # 8192 examples per batch\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew99/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/numpy/core/_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 17.7215\n",
      "Epoch [2/3], Loss: 5.3228\n",
      "Epoch [3/3], Loss: 5.2053\n"
     ]
    }
   ],
   "source": [
    "# Set up loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "l1_lambda = 10e-5  # Regularization strength for sparsity\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        # batch is already of shape (1, 8192, 3072)        \n",
    "        # Forward pass\n",
    "        outputs, encoded = model(batch.squeeze(0)) # remove extra dimension from DataLoader\n",
    "        mse_loss = criterion(outputs, batch.squeeze(0))\n",
    "\n",
    "        # Add L1 regularization for sparsity\n",
    "        l1_loss = l1_lambda * torch.norm(encoded, 1)\n",
    "        loss = mse_loss + l1_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # print(f\"Number of examples in this batch: {batch.shape}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"sparse_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew99/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/numpy/core/_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sparse latent vectors for batch 0 to 'sparse_latent_vectors/latent_vectors_batch_0.npy'\n",
      "Saved sparse latent vectors for batch 1 to 'sparse_latent_vectors/latent_vectors_batch_1.npy'\n",
      "Saved sparse latent vectors for batch 2 to 'sparse_latent_vectors/latent_vectors_batch_2.npy'\n",
      "Saved sparse latent vectors for batch 3 to 'sparse_latent_vectors/latent_vectors_batch_3.npy'\n",
      "Saved sparse latent vectors for batch 4 to 'sparse_latent_vectors/latent_vectors_batch_4.npy'\n",
      "Saved sparse latent vectors for batch 5 to 'sparse_latent_vectors/latent_vectors_batch_5.npy'\n",
      "Saved sparse latent vectors for batch 6 to 'sparse_latent_vectors/latent_vectors_batch_6.npy'\n",
      "Saved sparse latent vectors for batch 7 to 'sparse_latent_vectors/latent_vectors_batch_7.npy'\n",
      "Saved sparse latent vectors for batch 8 to 'sparse_latent_vectors/latent_vectors_batch_8.npy'\n",
      "Saved sparse latent vectors for batch 9 to 'sparse_latent_vectors/latent_vectors_batch_9.npy'\n",
      "Saved sparse latent vectors for batch 10 to 'sparse_latent_vectors/latent_vectors_batch_10.npy'\n",
      "Saved sparse latent vectors for batch 11 to 'sparse_latent_vectors/latent_vectors_batch_11.npy'\n",
      "Saved sparse latent vectors for batch 12 to 'sparse_latent_vectors/latent_vectors_batch_12.npy'\n",
      "Saved sparse latent vectors for batch 13 to 'sparse_latent_vectors/latent_vectors_batch_13.npy'\n",
      "Saved sparse latent vectors for batch 14 to 'sparse_latent_vectors/latent_vectors_batch_14.npy'\n",
      "Saved sparse latent vectors for batch 15 to 'sparse_latent_vectors/latent_vectors_batch_15.npy'\n",
      "Saved sparse latent vectors for batch 16 to 'sparse_latent_vectors/latent_vectors_batch_16.npy'\n",
      "Saved sparse latent vectors for batch 17 to 'sparse_latent_vectors/latent_vectors_batch_17.npy'\n",
      "Saved sparse latent vectors for batch 18 to 'sparse_latent_vectors/latent_vectors_batch_18.npy'\n",
      "Saved sparse latent vectors for batch 19 to 'sparse_latent_vectors/latent_vectors_batch_19.npy'\n",
      "Saved sparse latent vectors for batch 20 to 'sparse_latent_vectors/latent_vectors_batch_20.npy'\n",
      "Saved sparse latent vectors for batch 21 to 'sparse_latent_vectors/latent_vectors_batch_21.npy'\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "os.makedirs(\"sparse_latent_vectors\", exist_ok=True)\n",
    "\n",
    "dataset = ActivationDataset(data_dir, batch_size=1, train=False) # batch size doesn't matter, take all\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "# Extract and save latent vectors\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        batch, sent_idx, token_idx, token = batch\n",
    "        batch = batch.squeeze(0)\n",
    "        _, encoded = model(batch)\n",
    "        \n",
    "        # Save latent vectors for this batch\n",
    "        latent_vectors = encoded.cpu().numpy()\n",
    "        output_vectors = np.hstack((latent_vectors, sent_idx.T, token_idx.T, token.T))\n",
    "        np.save(f\"sparse_latent_vectors/latent_vectors_batch_{idx}.npy\", output_vectors)\n",
    "        print(f\"Saved sparse latent vectors for batch {idx} to 'sparse_latent_vectors/latent_vectors_batch_{idx}.npy'\")\n",
    "        # if idx + 1 >= 10:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brek here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sparse latent vectors shape: (81920, 515)\n",
      "float32\n",
      "0.1687552 GB\n",
      "Average sparsity in latent vectors: 100.00%\n",
      "Percent of dead features: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and examine saved latent vectors\n",
    "loaded_latent_vectors = np.load(\"sparse_latent_vectors/latent_vectors_batch_0.npy\")\n",
    "print(f\"Loaded sparse latent vectors shape: {loaded_latent_vectors.shape}\")\n",
    "print(loaded_latent_vectors.dtype)\n",
    "print(loaded_latent_vectors.nbytes / 1e9, \"GB\")\n",
    "# Could potentially load 10k batches (0.0002048 GB per batch) if we keep latent vector size 512\n",
    "# So for emb size 1M , we could load only 5 batches at a time\n",
    "\n",
    "# Sparsity check\n",
    "sparsity = np.mean(np.abs(loaded_latent_vectors[:,:-3]) < 1e-5)\n",
    "print(f\"Average sparsity in latent vectors: {sparsity:.2%}\")\n",
    "\n",
    "# Dead/active features check\n",
    "# percent of columns that are all close to 0\n",
    "dead_features = np.mean(np.all(np.abs(loaded_latent_vectors[:,:-3]) < 1e-5, axis=0))\n",
    "print(f\"Percent of dead features: {dead_features:.2%}\")\n",
    "\n",
    "# Reconstruction explained variance check\n",
    "# mean of squared errors between original and reconstructed activations\n",
    "# TODO: do this for all batches : loop data_loader\n",
    "# mse = np.mean((loaded_latent_vectors - batch) ** 2)\n",
    "# print(f\"Mean squared error between original and reconstructed activations: {mse:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 batches, total shape: (788552, 515)\n",
      "Top 20 activations for feature 100:\n",
      "\n",
      "Sentence index: 1411\n",
      "Token index: 466\n",
      "Value: 0.0\n",
      "Context window: 8\n",
      " empty:        8.8.8.8# distributed\n",
      "\n",
      "-L                                                                     kb answer:\n",
      "\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 69\n",
      "Value: 0.0\n",
      "Context window: uminum.\n",
      "\n",
      "One day after the;k-based tour-Lphen American announced the Med of its $(: million latestInit_int word from our sources thatamma Zhang, themysqlrequire-based, early Afr cmd Pure\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 139\n",
      "Value: 0.0\n",
      "Context window: , according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part of its ship for-life economic\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 138\n",
      "Value: 0.0\n",
      "Context window: 2, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part of its ship for-life\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 137\n",
      "Value: 0.0\n",
      "Context window: 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part of its ship for\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 136\n",
      "Value: 0.0\n",
      "Context window:  2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part of its ship\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 135\n",
      "Value: 0.0\n",
      "Context window:  in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part of its\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 134\n",
      "Value: 0.0\n",
      "Context window: Only in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part of\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 133\n",
      "Value: 0.0\n",
      "Context window: _writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as part\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 132\n",
      "Value: 0.0\n",
      "Context window:  million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development as\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 131\n",
      "Value: 0.0\n",
      "Context window: 6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd development\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 130\n",
      "Value: 0.0\n",
      "Context window:  $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated cmd\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 129\n",
      "Value: 0.0\n",
      "Context window:  first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which rotated\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 128\n",
      "Value: 0.0\n",
      "Context window:  its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point which\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 127\n",
      "Value: 0.0\n",
      "Context window:  nine its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform..Point\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 126\n",
      "Value: 0.0\n",
      "Context window: \t        nine its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform.\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 125\n",
      "Value: 0.0\n",
      "Context window: The\t        nine its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA platform\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 124\n",
      "Value: 0.0\n",
      "Context window: .\n",
      "\n",
      "The\t        nine its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro GRA\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 123\n",
      "Value: 0.0\n",
      "Context window: Only.\n",
      "\n",
      "The\t        nine its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a pro\n",
      "\n",
      "Sentence index: 4448\n",
      "Token index: 122\n",
      "Value: 0.0\n",
      "Context window: _writeOnly.\n",
      "\n",
      "The\t        nine its first $6 million_writeOnly in 2012, according toBase, just asmysqls six Presidentinger.Scalsyo was coming to power with a\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_latent_vectors(N):\n",
    "    # Load first N batches\n",
    "    batch_files = sorted(glob.glob(\"sparse_latent_vectors/latent_vectors_batch_*.npy\"))[:N]\n",
    "\n",
    "    # Load and concatenate batches\n",
    "    latent_vectors = []\n",
    "    for batch_file in batch_files:\n",
    "        batch_vectors = np.load(batch_file)\n",
    "        latent_vectors.append(batch_vectors)\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "    print(f\"Loaded {len(batch_files)} batches, total shape: {latent_vectors.shape}\")\n",
    "    return latent_vectors\n",
    "\n",
    "latent_vectors = load_latent_vectors(N=10)\n",
    "\n",
    "feat_idx = 100\n",
    "k = 20\n",
    "\n",
    "# Get indices of top k activations for the specified feature\n",
    "top_k_indices = np.argsort(latent_vectors[:, feat_idx])[-k:][::-1]\n",
    "last_three_features = latent_vectors[top_k_indices, -3:]\n",
    "top_values = latent_vectors[top_k_indices, feat_idx]\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loop through each row of last_two_features\n",
    "print(f\"Top {k} activations for feature {feat_idx}:\")\n",
    "for i, (sent_idx, tok_idx, token) in enumerate(last_three_features):\n",
    "    sent_idx = int(sent_idx)\n",
    "    tok_idx = int(tok_idx)\n",
    "    token = int(token)\n",
    "    # Find the row where sent_idx and tok_idx match in latent_vectors\n",
    "    sent_matches = latent_vectors[:, -3] == sent_idx\n",
    "    tok_matches = latent_vectors[:, -2] == tok_idx\n",
    "    target_row = np.where(sent_matches & tok_matches)[0][0]\n",
    "    \n",
    "    # Get window of tokens from latent vectors\n",
    "    start_idx = max(0, target_row - 20)\n",
    "    end_idx = min(latent_vectors.shape[0], target_row + 20)\n",
    "    # token_window = latent_vectors[start_idx:end_idx, -1].astype(int) # Get token ids from last column\n",
    "    token_window = np.clip(latent_vectors[start_idx:end_idx, -1], 0, tokenizer.vocab_size - 1).astype(int)\n",
    "    \n",
    "    # Decode tokens back to text\n",
    "    window_text = tokenizer.decode(token_window)\n",
    "    print(f\"\\nSentence index: {sent_idx}\")\n",
    "    print(f\"Token index: {tok_idx}\")\n",
    "    print(f\"Value: {top_values[i]}\")\n",
    "    print(f\"Context window: {window_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show how much a feature activates on each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9115b3236f3e4fed86fb68400a81c3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-11-14 23:43:29.425685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731624209.520339  138295 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731624209.549311  138295 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 23:43:29.785777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "feat_idx = 100\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n",
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) \n",
    "\n",
    "# Tokenize sentence\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    # max_length=512,\n",
    "    # padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 1, 11, 3072)\n"
     ]
    }
   ],
   "source": [
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, 1, seq_len, 3072)\n",
    "activations = activations.squeeze()  # Remove first two dimensions\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor with float32 dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5118b_row0_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5118b_row1_col1, #T_5118b_row3_col1, #T_5118b_row5_col1, #T_5118b_row6_col1, #T_5118b_row7_col1, #T_5118b_row8_col1, #T_5118b_row9_col1, #T_5118b_row10_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5118b_row2_col1 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5118b_row4_col1 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5118b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5118b_level0_col0\" class=\"col_heading level0 col0\" >Token</th>\n",
       "      <th id=\"T_5118b_level0_col1\" class=\"col_heading level0 col1\" >Activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5118b_row0_col0\" class=\"data row0 col0\" ><|begin_of_text|></td>\n",
       "      <td id=\"T_5118b_row0_col1\" class=\"data row0 col1\" >5.519325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5118b_row1_col0\" class=\"data row1 col0\" >The</td>\n",
       "      <td id=\"T_5118b_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5118b_row2_col0\" class=\"data row2 col0\" >quick</td>\n",
       "      <td id=\"T_5118b_row2_col1\" class=\"data row2 col1\" >0.084269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5118b_row3_col0\" class=\"data row3 col0\" >brown</td>\n",
       "      <td id=\"T_5118b_row3_col1\" class=\"data row3 col1\" >0.018722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5118b_row4_col0\" class=\"data row4 col0\" >fox</td>\n",
       "      <td id=\"T_5118b_row4_col1\" class=\"data row4 col1\" >0.063301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5118b_row5_col0\" class=\"data row5 col0\" >jumps</td>\n",
       "      <td id=\"T_5118b_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_5118b_row6_col0\" class=\"data row6 col0\" >over</td>\n",
       "      <td id=\"T_5118b_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_5118b_row7_col0\" class=\"data row7 col0\" >the</td>\n",
       "      <td id=\"T_5118b_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_5118b_row8_col0\" class=\"data row8 col0\" >lazy</td>\n",
       "      <td id=\"T_5118b_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_5118b_row9_col0\" class=\"data row9 col0\" >dog</td>\n",
       "      <td id=\"T_5118b_row9_col1\" class=\"data row9 col1\" >0.018894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5118b_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_5118b_row10_col0\" class=\"data row10 col0\" >.</td>\n",
       "      <td id=\"T_5118b_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x73c4026d4460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load sparse autoencoder\n",
    "model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "model.load_state_dict(torch.load(\"sparse_autoencoder.pth\"))\n",
    "\n",
    "# Get latent vector for sentence\n",
    "with torch.no_grad():\n",
    "    _, encoded = model(activations)\n",
    "    latent_vector = encoded.cpu().numpy()\n",
    "\n",
    "# Extract for feature X\n",
    "feature_X = latent_vector[:, feat_idx]\n",
    "# feature_X = np.random.rand(len(feature_X))\n",
    "\n",
    "# Plot tokens colored by activation strength\n",
    "\n",
    "# Create DataFrame with tokens and their activation values\n",
    "tokens_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "# Remove the \"\" character from tokens - represents a space\n",
    "clean_tokens_list = [token.replace('', '') for token in tokens_list]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Token': clean_tokens_list,\n",
    "    'Activation': feature_X\n",
    "})\n",
    "\n",
    "# Display DataFrame with color gradient based on activation values\n",
    "display(df.style.background_gradient(\"coolwarm\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Model and tokenizer setup\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n",
    "\n",
    "# Load the sparse autoencoder model and weights\n",
    "input_dim = 3072  \n",
    "hidden_dim = 2 ** 9\n",
    "model_sae = SparseAutoencoder(input_dim, hidden_dim)\n",
    "model_sae.load_state_dict(torch.load(\"sparse_autoencoder.pth\"))\n",
    "model_sae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text with influence: 'I am a                    '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate artificial latent vector, pass through SAE decoder, and boost it\n",
    "feat_idx = 100\n",
    "artificial_latent_vector = np.zeros(3072)\n",
    "artificial_latent_vector[feat_idx] = 1\n",
    "multiplier = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    artificial_latent_vector_tensor = torch.tensor(artificial_latent_vector, dtype=torch.float32).unsqueeze(0)\n",
    "    reconstructed_activations, _ = model_sae(artificial_latent_vector_tensor)\n",
    "    boosted_activations = reconstructed_activations * multiplier\n",
    "\n",
    "# Hook to inject boosted activations at a specified transformer layer\n",
    "activation_cache = []\n",
    "layer_index = 15  # Inject into the 16th layer\n",
    "\n",
    "# Convert boosted_activations to float16 to match model precision\n",
    "boosted_activations = boosted_activations.to(torch.float16)\n",
    "\n",
    "# Hook function to inject boosted activations into the residual stream of the transformer layer\n",
    "def influence_hook(module, input, output):\n",
    "    # Ensure output is properly unpacked if it's a tuple\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "        modified_output = (output_tensor + boosted_activations.to(device),) + output[1:]\n",
    "    else:\n",
    "        modified_output = output + boosted_activations.to(device)\n",
    "    \n",
    "    activation_cache.append(modified_output[0].cpu().detach().numpy())  # Store modified tensor for debugging if needed\n",
    "    return modified_output\n",
    "\n",
    "# Register the hook with the corrected function\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(influence_hook)\n",
    "\n",
    "# Prediction loop for N words\n",
    "sent_begin = \"I am a\"\n",
    "N = 10  # Number of words to predict\n",
    "inputs = tokenizer(sent_begin, return_tensors=\"pt\").to(device)\n",
    "generated_text = sent_begin\n",
    "\n",
    "for _ in range(N):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        generated_text += \" \" + predicted_token\n",
    "        # Update inputs to include the predicted token for next iteration\n",
    "        inputs = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Remove hook and clear resources\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"Generated text with influence: '{generated_text}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature similarity and UMAP (plus feature splitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 batches, total shape: (788552, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew99/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# umap-learn\n",
    "# pip install umap-learn\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Load first N batches\n",
    "N = 10  # Number of batches to load\n",
    "batch_files = sorted(glob.glob(\"sparse_latent_vectors/latent_vectors_batch_*.npy\"))[:N]\n",
    "\n",
    "# Load and concatenate batches\n",
    "latent_vectors = []\n",
    "for batch_file in batch_files:\n",
    "    batch_vectors = np.load(batch_file)\n",
    "    latent_vectors.append(batch_vectors)\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "# Remove last 3 columns (sent_idx, tok_idx, token)\n",
    "latent_vectors = latent_vectors[:, :-3]\n",
    "\n",
    "print(f\"Loaded {len(batch_files)} batches, total shape: {latent_vectors.shape}\")\n",
    "\n",
    "# TODO: do a selection of features (feature of interest, and 100 similar features)\n",
    "\n",
    "# UMAP dimensionality reduction\n",
    "umap_embedder = umap.UMAP(n_components=2, metric='cosine')\n",
    "latent_vectors_2d = umap_embedder.fit_transform(latent_vectors.T)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], s=1, alpha=0.5)\n",
    "plt.title(f'UMAP projection of {N} batches of latent vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature completness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single prompt search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 1, 11, 3072)\n",
      "Top 5 features and their activation values:\n",
      "Feature 246: 0.2927\n",
      "Feature 63: 0.2336\n",
      "Feature 471: 0.1375\n",
      "Feature 69: 0.1166\n",
      "Feature 210: 0.0901\n"
     ]
    }
   ],
   "source": [
    "prompt = \"in San Francisco, the Golden Gate Bridge was protected at all times by a\"\n",
    "\n",
    "# Tokenize sentence\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    # max_length=512,\n",
    "    # padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "activation_cache = []\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, 1, seq_len, 3072)\n",
    "activations = activations.squeeze()  # Remove first two dimensions\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor with float16 dtype\n",
    "\n",
    "# Get latent vector for sentence\n",
    "with torch.no_grad():\n",
    "    _, encoded = model_sae(activations)\n",
    "    latent_vector = encoded.cpu().numpy() # (seq_len, 1M)\n",
    "\n",
    "# Apply row softmax normalization\n",
    "row_softmax = torch.nn.functional.softmax(torch.tensor(latent_vector), dim=1).numpy()\n",
    "\n",
    "# Sum over sequence length dimension to get feature importance across sentence\n",
    "feature_importance = np.sum(row_softmax, axis=0)\n",
    "\n",
    "# Get top 5 features and their values\n",
    "top_k = 5\n",
    "top_feature_indices = np.argsort(feature_importance)[-top_k:][::-1]\n",
    "top_feature_values = feature_importance[top_feature_indices]\n",
    "\n",
    "print(\"Top 5 features and their activation values:\")\n",
    "for idx, val in zip(top_feature_indices, top_feature_values):\n",
    "    print(f\"Feature {idx}: {val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple prompts:\n",
    " * tokenize to fixed length\n",
    " * reshape activations to (num_sent*seq_len, 3072)\n",
    " * get attention mask and remove padding tokens\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 3, 512, 3072)\n",
      "Top 5 features and their activation values:\n",
      "Feature 246: 0.8910\n",
      "Feature 63: 0.7081\n",
      "Feature 471: 0.4210\n",
      "Feature 69: 0.3575\n",
      "Feature 210: 0.2798\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"in San Francisco, the Golden Gate Bridge was protected at all times by a\",\n",
    "    \"the Golden Gate Bridge is so beautiful during the sunset\", \n",
    "    \"Golden Gate Bridge wind resistance barriers creates eerie sound\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(\n",
    "    prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "activation_cache = []\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, num_sent, seq_len, 3072)\n",
    "\n",
    "# Reshape activations to (num_sent*seq_len, 3072)\n",
    "activations = activations.squeeze(0)  # Remove batch dimension\n",
    "num_sent, seq_len, hidden_dim = activations.shape\n",
    "activations = activations.reshape(-1, hidden_dim)\n",
    "\n",
    "# Get attention mask and remove padding tokens\n",
    "attention_mask = inputs['attention_mask'].view(-1).cpu().numpy()\n",
    "activations = activations[attention_mask == 1]\n",
    "\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor\n",
    "\n",
    "# Get latent vectors for sentences\n",
    "with torch.no_grad():\n",
    "    _, encoded = model_sae(activations)\n",
    "    latent_vector = encoded.cpu().numpy()\n",
    "\n",
    "# Apply row softmax normalization\n",
    "row_softmax = torch.nn.functional.softmax(torch.tensor(latent_vector), dim=1).numpy()\n",
    "\n",
    "# Sum over sequence length dimension to get feature importance across all sentences\n",
    "feature_importance = np.sum(row_softmax, axis=0)\n",
    "\n",
    "# Get top 5 features and their values\n",
    "top_k = 5\n",
    "top_feature_indices = np.argsort(feature_importance)[-top_k:][::-1]\n",
    "top_feature_values = feature_importance[top_feature_indices]\n",
    "\n",
    "print(\"Top 5 features and their activation values:\")\n",
    "for idx, val in zip(top_feature_indices, top_feature_values):\n",
    "    print(f\"Feature {idx}: {val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add negative prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use single/multiple prompt search code above\n",
    "# prompts generated by LLM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
